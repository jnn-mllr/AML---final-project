{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b46837",
   "metadata": {},
   "source": [
    "# Adult Census Income Prediction\n",
    "\n",
    "## Project Goal\n",
    "\n",
    "The goal here is to predict whether someone makes more than $50,000/year using census data.  \n",
    "this notebook walks through the whole pipeline: loading, cleaning, exploring, feature engineering, splitting, scaling, modeling, and evaluation.  \n",
    "we use the custom courselib library together with newly created custom functions and try to show clear visuals and explanations at each step.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "1.  **Load & Clean Data**\n",
    "2.  **Exploratory Data Analysis (EDA)**\n",
    "3.  **Feature Engineering & Preprocessing**\n",
    "    -   3.1 Category Merging\n",
    "    -   3.2 Encode categorical variables (target, ordinal, one-hot, frequency)\n",
    "    -   3.3 Feature Correlations\n",
    "    -   3.4 Splitting the Data\n",
    "    -   3.5 Scale Numerical Features\n",
    "4.  **Model Training**\n",
    "    -   4.1 Base model\n",
    "    -   4.2 \n",
    "    -   4.3 \n",
    "5.  **Hyperparameter Tuning**\n",
    "6.  **Model Evaluation & Comparison**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86828215",
   "metadata": {},
   "source": [
    "## 1 Load & Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cab40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreloading of imported modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Add the repo root to access the courselib\n",
    "repo_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "courselib_path = os.path.join(repo_root, \"AppliedML\", \"courselib\")\n",
    "if courselib_path not in sys.path:\n",
    "    sys.path.insert(0, courselib_path)\n",
    "    print(f\"{courselib_path} added to sys.path.\")\n",
    "else:\n",
    "    print(\"Courselib path already in sys.path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d08ae2",
   "metadata": {},
   "source": [
    "As this project is supposed to integrate well with the courselib, we have downloaded the current GitHub Repo up to week 11 and our code will be integrated within courselib libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c9a498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loaders import load_uciadult\n",
    "\n",
    "# ensure the data directory exists / else create it\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# get the data\n",
    "df = load_uciadult()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477c3dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f8868c",
   "metadata": {},
   "source": [
    "1.  **Handle Duplicates**: We remove any duplicate rows from the dataset.\n",
    "2.  **Handle Missing Values**: Instead of dropping rows with missing values, we treat them as the separate category  `Missing` in categorical columns. This allows us to preserve potentially useful information â€” for example, individuals with unknown workclass or occupation might share certain income patterns. This approach is especially appropriate for categorical variables, where missingness itself may contain information. Fortunately, this data set does not contain other missing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d11a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import preprocess_data\n",
    "\n",
    "# preprocessing of  the data\n",
    "df = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1208affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first glimpse into the data set\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e7d854",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0c67be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic info about the data set \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b7ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary stats about the variables\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa1c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplots of the numerical cols \n",
    "numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'income' in numerical_cols:\n",
    "    numerical_cols.remove('income')\n",
    "\n",
    "# grid of box plots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(numerical_cols), figsize=(20, 6))\n",
    "fig.suptitle('Box Plots of Numerical Variables', fontsize=16)\n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    ax = axes[i]\n",
    "    ax.boxplot(df[col], patch_artist=True, boxprops=dict(facecolor='#00246B'))\n",
    "    ax.set_title(col, fontsize=12)\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3973d568",
   "metadata": {},
   "source": [
    "- High Skewness: `fnlwgt`, `capital-gain`, and `capital-loss` are heavily right-skewed. For `capital-gain`, and `capital-loss`, the majority of values are zero, with a few very large outliers. This indicates that these events are rare but have a high magnitude when they occur. `fnlwgt`is dropped as that variable is difficult to interpret. (It indicates how many people in the population a particular record represents and is therefore not an individual attribute and also highly skewed with high variance.)\n",
    "- Moderate Outliers: age and hours-per-week also show outliers but have more symmetric distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7a6fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import log_transform\n",
    "\n",
    "# skewed columns and apply log transformation to have them on a more similar scale\n",
    "skewed_cols = ['capital-gain', 'capital-loss']\n",
    "df = log_transform(df, skewed_cols)\n",
    "df.drop('fnlwgt', axis=1, inplace=True) # fnlwgt is difficult to interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336cd763",
   "metadata": {},
   "source": [
    "The following chart shows a class imbalance in the data: approximately 76% of individuals earn <=50K (low-earners = 0) while only 24% earn >50K (high-earners = 1). A model trained on this data might become biased towards predicting the majority class. Standard accuracy can be a misleading metric; a naive model that always predicts <=50K would be ~76% accurate but completely useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666d1275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of the target variable\n",
    "plt.figure(figsize=(7, 5))\n",
    "bars = df['income'].value_counts().sort_index().plot(\n",
    "    kind='bar',\n",
    "    color=['#00246B', '#CADCFC'],\n",
    ")\n",
    "plt.title('Class Balance of Income', fontsize=16)\n",
    "plt.xlabel('Income (0 = <=50K, 1 = >50K)', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks([0, 1], ['<=50K', '>50K'], rotation=0, fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "counts = df['income'].value_counts().sort_index()\n",
    "labels = ['<=50K', '>50K']\n",
    "for i, count in enumerate(counts):\n",
    "    pct = count / counts.sum() * 100\n",
    "    plt.text(i, count + 200, f'{pct:.1f}%', ha='center', fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3b623d",
   "metadata": {},
   "source": [
    "The age distribution shows that individuals earning over $50K are, on average, older than those earning less. The high-income group's distribution peaks between the late 30s and early 50s, suggesting that income potential increases with age and experience before declining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f345c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relationship between income and age\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# separate data for the two income categories\n",
    "over50k = df[df['income'] == 0]['age']\n",
    "below50k = df[df['income'] == 1]['age']\n",
    "\n",
    "# side-by-side histograms\n",
    "plt.hist([over50k, below50k], bins=20, color=['#00246B', '#CADCFC'], label=['<=50K', '>50K'])\n",
    "plt.title('Age Distribution by Income Level', fontsize=16)\n",
    "plt.xlabel('Age', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.legend(title='Income')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baae0db3",
   "metadata": {},
   "source": [
    "There seems to exist a strong, positive correlation between education and income. The likelihood of earning over $50K increases directly with the level of education, which seems economically reasonable. This clear ordering suggests the use of ordinal encoding for this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8c92dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "education_income_pct = df.groupby('education', observed=False)['income'].mean().sort_values() * 100\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = education_income_pct.plot(kind='barh', color='#00246B')\n",
    "plt.title('Percentage of Individuals Earning >$50K by Education Level', fontsize=16)\n",
    "plt.xlabel('Percentage (%)', fontsize=12)\n",
    "plt.ylabel('Education Level', fontsize=12)\n",
    "plt.xticks(fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "# percentage labels\n",
    "for bar in bars.patches:\n",
    "    plt.text(bar.get_width() + 0.1,\n",
    "             bar.get_y() + bar.get_height() / 2,\n",
    "             f'{bar.get_width():.1f}%',\n",
    "             va='center',\n",
    "             ha='left',\n",
    "             fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29275c67",
   "metadata": {},
   "source": [
    "This chart shows the average hours worked per week for each workclass, with bar color indicating the proportion of high-income earners. `Self-emp-inc` stands out with both the highest average hours and the greatest income potential (55.7%). Conversely, categories like `Without-pay` and `Never-worked` show zero high-income potential. The `Missing` category also has a low income potential (10.4%), suggesting that the reason for the missing data might be correlated with lower-paying or non-standard work situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661c9095",
   "metadata": {},
   "outputs": [],
   "source": [
    "workclass_stats = df.groupby('workclass', observed=False).agg(\n",
    "    mean_hours=('hours-per-week', 'mean'),\n",
    "    pct_high_income=('income', 'mean')\n",
    ")\n",
    "overall_stats = pd.DataFrame({\n",
    "    'mean_hours': [df['hours-per-week'].mean()],\n",
    "    'pct_high_income': [df['income'].mean()]\n",
    "}, index=['Overall Average'])\n",
    "\n",
    "# merge workclass stats with overall stats\n",
    "combined_stats = pd.concat([workclass_stats, overall_stats]).sort_values('mean_hours', ascending=False)\n",
    "\n",
    "# plot and colormap\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "cmap = plt.get_cmap('Blues')\n",
    "norm = plt.Normalize(vmin=combined_stats['pct_high_income'].min(), vmax=combined_stats['pct_high_income'].max())\n",
    "colors = cmap(norm(combined_stats['pct_high_income'].values))\n",
    "bars = ax.bar(combined_stats.index, combined_stats['mean_hours'], color=colors)\n",
    "\n",
    "# color bar for the mean proportion of high-earners\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = fig.colorbar(sm, ax=ax, pad=0.05)\n",
    "cbar.set_label('Proportion Earning >$50K', fontsize=12)\n",
    "\n",
    "# text labels for the proportions\n",
    "labels = [f'{pct*100:.1f}%' for pct in combined_stats['pct_high_income']]\n",
    "ax.bar_label(bars, labels=labels, padding=3, fontsize=10, color='black')\n",
    "ax.set_title('Mean Hours Worked by Workclass (vs. Overall Average)', fontsize=16)\n",
    "ax.set_xlabel('Work Class / Category', fontsize=12)\n",
    "ax.set_ylabel('Mean Hours per Week', fontsize=12)\n",
    "ax.set_ylim(0, combined_stats['mean_hours'].max() * 1.1)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08bc11c",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdf9b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts and proportions of every categorical col\n",
    "categorical_cols_merged = df.select_dtypes(include=['object', 'category']).columns\n",
    "for col in categorical_cols_merged:\n",
    "    print(f\"--- Feature: {col} ---\")\n",
    "    \n",
    "    # summary of each group within a col\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Count': df[col].value_counts(),\n",
    "        'Proportion (%)': (df[col].value_counts(normalize=True) * 100).round(2)\n",
    "    })\n",
    "    print(summary_df)\n",
    "    print(\"_\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd7aa70",
   "metadata": {},
   "source": [
    "### 3.1 Category Merging\n",
    "\n",
    "Before encoding, we will merge some categories to reduce dimensionality and group similar items to simplify features and help the model generalize better. Thus several possible categorical variables are analyzed and discussed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd06d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to modify the dataset\n",
    "df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cca5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary feature for US vs. rest of the world as the dataset is mostly US-related\n",
    "df['is_from_us'] = (df['native-country'] == 'United-States').astype(int)\n",
    "df.drop('native-country', axis=1, inplace=True)\n",
    "print(\"Value counts for 'is_from_us':\\n\")\n",
    "print(df['is_from_us'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d85722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pie chart for US vs. Non-US\n",
    "us_vs_nonus = df.groupby('is_from_us')['income'].mean()\n",
    "labels = ['Non-US', 'US']\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.pie(us_vs_nonus, labels=labels, autopct='%1.1f%%', colors=['#CADCFC', '#00246B'], startangle=90)\n",
    "plt.title('Proportion Earning >$50K: US vs. Non-US')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c5aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean income by marital status\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Count of each marital status\n",
    "marital_counts = df['marital-status'].value_counts().sort_values(ascending=False)\n",
    "marital_counts.plot(kind='bar', color='#00246B', ax=axes[0])\n",
    "axes[0].set_title('Distribution of Marital Status')\n",
    "axes[0].set_xlabel('Marital Status')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=30)\n",
    "\n",
    "# Mean income by marital status\n",
    "marital_income = df.groupby('marital-status', observed=False)['income'].mean().sort_values(ascending=False) * 100\n",
    "marital_income.plot(kind='bar', color='#CADCFC', ax=axes[1])\n",
    "axes[1].set_title('Mean Income >$50K by Marital Status')\n",
    "axes[1].set_xlabel('Marital Status')\n",
    "axes[1].set_ylabel('Proportion >$50K (%)')\n",
    "axes[1].tick_params(axis='x', rotation=30)\n",
    "for i, v in enumerate(marital_income):\n",
    "    axes[1].text(i, v + 0.5, f\"{v:.1f}%\", ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c343d51e",
   "metadata": {},
   "source": [
    "The plot suggests that creating a binary feature for married vs. non-married, although we will treat the group Married-spouse-absent, i.e. legally married but not living with spouse (spouse is absent, e.g., separated but not divorced) as non-married."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68707943",
   "metadata": {},
   "outputs": [],
   "source": [
    "married_statuses = ['Married-civ-spouse', 'Married-AF-spouse']\n",
    "df['is_married'] = df['marital-status'].isin(married_statuses).astype(int)\n",
    "df.drop('marital-status', axis=1, inplace=True)\n",
    "print(\"\\nValue counts for the new 'is_married' feature:\\n\")\n",
    "print(df['is_married'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146804ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge husband and wife into spouse in relationship variable to further reduce cardinality for onehot encoding\n",
    "df['relationship'] = df['relationship'].astype(str).replace({'Husband': 'Spouse', 'Wife': 'Spouse'}).astype('category')\n",
    "\n",
    "# merge without-pay, never-worked categories into other to further reduce cardinality for onehot encoding\n",
    "df['workclass'] = df['workclass'].astype(str).replace(['Without-pay', 'Never-worked'], 'Other').astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3bb598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# race distribution and mean income by race\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# count of each race\n",
    "race_counts = df['race'].value_counts().sort_values(ascending=False)\n",
    "race_counts.plot(kind='bar', color='#00246B', ax=axes[0])\n",
    "axes[0].set_title('Distribution of Race')\n",
    "axes[0].set_xlabel('Race')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=30)\n",
    "\n",
    "# mean income by race\n",
    "race_income = df.groupby('race', observed=False)['income'].mean().sort_values(ascending=False) * 100\n",
    "race_income.plot(kind='bar', color='#CADCFC', ax=axes[1])\n",
    "axes[1].set_title('Mean Income >$50K by Race')\n",
    "axes[1].set_xlabel('Race')\n",
    "axes[1].set_ylabel('Proportion >$50K (%)')\n",
    "axes[1].tick_params(axis='x', rotation=30)\n",
    "for i, v in enumerate(race_income):\n",
    "    axes[1].text(i, v + 0.2, f\"{v:.1f}%\", ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9696c99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge Amer-Indian-Eskimo and Other categories into other to further reduce cardinality for onehot encoding\n",
    "df['race'] = df['race'].astype(str).replace({'Amer-Indian-Eskimo': 'Other'}).astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d753e44e",
   "metadata": {},
   "source": [
    "The merging of categories in features like race, relationship, and workclass leads to only a small reduction in dimensionality, but is justified because the combined groups are both small and statistically similar in terms of income distribution. This reduces noise and the risk of overfitting, while preserving the main socioeconomic patterns in the data. By merging only where appropriate, we maintain interpretability and model performance without unnecessary simplification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb8ba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of distinct values in our categorical cols -> what feature encoding strat to use\n",
    "categorical_vars = ['workclass', 'relationship', 'race', 'sex', 'education', 'occupation']\n",
    "\n",
    "for var in categorical_vars:\n",
    "    if var in df.columns:\n",
    "        unique_vals = df[var].unique()\n",
    "        print(f\"{var}: {len(unique_vals)} unique values:\\n {list(unique_vals)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce7f0c3",
   "metadata": {},
   "source": [
    "### 3.2 Data Splitting\n",
    "\n",
    "To prevent data leakage, we must split the data into training and testing sets *before* any encoding or scaling. Previously, this mistake lead to artificially good performance on the validation set. All fitting steps (like for encoders and scalers) will be performed on the training set only and then applied to both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7907a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.splits import train_test_split\n",
    "\n",
    "# returns the full dataset (X, y)\n",
    "df, train_df, test_df = train_test_split(\n",
    "    df, \n",
    "    training_data_fraction=0.8, \n",
    "    class_column_name='income',\n",
    "    shuffle=True, \n",
    "    return_numpy=False # dataframes instead of numpy arrays\n",
    ")\n",
    "\n",
    "print(\"Train df shape:\", train_df.shape)\n",
    "print(\"Test df shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62b7162",
   "metadata": {},
   "source": [
    "### 3.3 Feature Encoding strategies\n",
    "\n",
    "Feature encoding transforms categorical variables into numerical values for machine learning models. We will apply one-hot and ordinal encoding first, followed by a careful application of target encoding to prevent data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e2d499",
   "metadata": {},
   "source": [
    "Brief explanation of all the strategies for encoding:\n",
    "\n",
    "#### 1. One-Hot Encoding\n",
    "\n",
    "- **Def.:** For a feature with $k$ categories, create $k-1$ binary columns:\n",
    "  $$\n",
    "  \\text{OneHot}(x = c_j) = [0, \\ldots, 1, \\ldots, 0] \\in \\mathbb{R}^{k-1}\n",
    "  $$\n",
    "  where the $j$-th position is $1$ if $x = c_j$, else $0$.\n",
    "- **Example:** If `race` has categories `[White, Black, Asian]` and $x = \\text{Black}$, then $\\text{OneHot}(x) = [1, 0]$.\n",
    "- **When/Why:** Use for nominal (unordered) features to avoid implying order. We create $k-1$ columns instead of $k$ to avoid **perfect multicollinearity**, where one feature can be perfectly predicted by the others. This is crucial for linear models. The dropped category  (the most frequent one) becomes the baseline reference.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Ordinal Encoding\n",
    "\n",
    "- **Def.:** Map ordered categories to integers:\n",
    "  $$\n",
    "  \\text{Ordinal}(x = c_j) = j\n",
    "  $$\n",
    "  where $j$ is the position in the predefined order.\n",
    "- **Example:** If `education` order is `[HS-grad, Bachelors, Masters]$ and $x = \\text{Bachelors}$, then $\\text{Ordinal}(x) = 1$.\n",
    "- **When/Why:** Use for ordinal features where order matters.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Target Encoding\n",
    "\n",
    "- **Def.:** Replace each category with the mean target value:\n",
    "  $$\n",
    "  \\text{TargetEnc}(x = c_j) = \\mathbb{E}[y \\mid x = c_j]\n",
    "  $$\n",
    "- **Example:** If mean income for `occupation = Tech` is $0.32$, then $\\text{TargetEnc}(\\text{Tech}) = 0.32$.\n",
    "- **When/Why:** Use for high-cardinality features; captures target signal but risk of leakage (use cross-validation).\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Frequency Encoding\n",
    "\n",
    "- **Def.:** Replace each category with its frequency:\n",
    "  $$\n",
    "  \\text{FreqEnc}(x = c_j) = \\frac{\\text{count}(x = c_j)}{N}\n",
    "  $$\n",
    "  where $N$ is the total number of samples.\n",
    "- **Example:** If `country = US` appears in $70\\%$ of rows, then $\\text{FreqEnc}(\\text{US}) = 0.7$.\n",
    "- **When/Why:** Use for high-cardinality features; simple, no leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc576ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import encode_and_align_features\n",
    "\n",
    "# encoding strategies for each column we need to encode\n",
    "base_encoding = {'one-hot': { 'relationship' : 'Spouse',\n",
    "                'race' : 'White', # second entry is dropped to avoid multicollinearity\n",
    "                'sex' : 'Male'},\n",
    "    'ordinal': {'education': ['Preschool', '1st-4th', '5th-6th', '7th-8th', '9th', '10th', '11th', '12th', 'HS-grad',\n",
    "                               'Some-college', 'Assoc-voc', 'Assoc-acdm', 'Bachelors', 'Masters', 'Prof-school',\n",
    "                               'Doctorate']},\n",
    "    'target': {'occupation': 'income'},  # Format: {feature_to_encode: target_column}\n",
    "    'frequency': ['workclass']\n",
    "}\n",
    "\n",
    "train_df_encoded, test_df_encoded, fit_params = encode_and_align_features(\n",
    "    train_df, test_df, base_encoding, drop_cols=['education-num'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c9d557",
   "metadata": {},
   "source": [
    "### 3.4 Correlation of Features with the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9c2bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar chart of correlations with income\n",
    "correlation_matrix_encoded = train_df_encoded.corr()\n",
    "income_correlations = correlation_matrix_encoded['income'].drop('income').sort_values()\n",
    "plt.figure(figsize=(8, 5))\n",
    "income_correlations.plot(kind='barh', color='#00246B')\n",
    "plt.title('Correlation of Features with Income in Training Data', fontsize=16)\n",
    "plt.xlabel('Pearson Correlation Coefficient', fontsize=12)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0db0aff",
   "metadata": {},
   "source": [
    "#### Interpretation:\n",
    "\n",
    "*   **Positive Correlation (Features on the right):** Features with a positive bar indicate that as the feature's value increases, the likelihood of having an income `>50K` also increases.\n",
    "*   **Negative Correlation (Features on the left):** Features with a negative bar indicate that as the feature's value increases, the likelihood of having an income `>50K` decreases.\n",
    "\n",
    "*   **Strongest Predictors:** `is_married`, `education` (ordinal), `age`, `hours-per-week`, and `capital-gain` show the strongest positive correlations. This aligns with our EDA findings: being married, having higher education, being older, and working more hours are all strongly associated with higher income. The target-encoded `occupation` feature is also a very strong predictor by design.\n",
    "\n",
    "*   **Negative Predictors:** The strongest negative correlations come from one-hot encoded features like `relationship_Own-child` and `relationship_Not-in-family`. This indicates that individuals in these relationship categories are significantly less likely to have high income.\n",
    "\n",
    "\n",
    "The dropped variables are the silent baseline against which all the other one-hot encoded columns from that same feature are measured. Their relationships are not directly visible in the matrix, but are implicitly defined by the relationships of the columns that are present. The relationship of the dropped `race_White` category is the inverse of the combined effect of the other categories. Because `race_White` is the most frequent category, the other, smaller race categories being negatively correlated with income implies that the large, baseline `race_White` category must have a correlation that is neutral or slightly positive to balance things out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f35c607",
   "metadata": {},
   "source": [
    "### 3.5 Data Splitting into features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4683b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.splits import split_features_and_target\n",
    "X_train, y_train, X_test, y_test = split_features_and_target(train_df_encoded, test_df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85245d97",
   "metadata": {},
   "source": [
    "### 3.6 Feature Scaling\n",
    "\n",
    "Before training our models, it's important to scale our numerical features. Of course, it makes no sense to also scale the freshly encoded categorical data, such as the ordinally encoded `education` variable as we would destroy its ordinal structure immediately again.  Models like Logistic Regression are sensitive to the scale of the input data, and scaling can lead to faster convergence and better performance. We will use standardization, where each feature is scaled to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "We will fit the scaler on the training data and use the same scaler to transform the test data. This prevents any information from the test set from leaking into our training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2bf8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all columns in the data set\n",
    "train_df_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b245df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.normalization import scale_numerical_features\n",
    "\n",
    "numerical_cols = [\n",
    "    'capital-gain', 'capital-loss', 'age', 'hours-per-week',\n",
    "    'education_ordinal', 'occupation_target', 'workclass_freq'\n",
    "]\n",
    "\n",
    "# scale features via normalization\n",
    "X_train_scaled, X_test_scaled = scale_numerical_features(\n",
    "    X_train, X_test, train_df_encoded, numerical_cols\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde812bc",
   "metadata": {},
   "source": [
    "## 4. Model Training (Logistic Regression)\n",
    "\n",
    "Predict whether an individual's income exceeds $50,000 per year.\n",
    "\n",
    "### Models Included:\n",
    "- **Logistic Regression**: A logistic Regression model for binary classification, optimized using gradient descent.\n",
    "\n",
    "### Evaluation Metrics:\n",
    "- **Accuracy**: The proportion of total correct predictions.\n",
    "  $$ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
    "- **Precision**: The accuracy of positive predictions.\n",
    "  $$ \\text{Precision} = \\frac{TP}{TP + FP} $$\n",
    "- **Recall (Sensitivity)**: The ability of the model to find all the positive samples.\n",
    "  $$ \\text{Recall} = \\frac{TP}{TP + FN} $$\n",
    "- **Training Time**: time taken to train the model to evaluate efficiency.\n",
    "- **Confusion Matrix**: A table visualizing the counts of TP, TN, FP, and FN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80395193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.glm import LogisticRegression\n",
    "from optimizers import GDOptimizer\n",
    "from utils.training import ModelTraining\n",
    "from utils.metrics import accuracy, precision_score, recall_score\n",
    "\n",
    "\n",
    "# model params\n",
    "n_features = X_train_scaled.shape[1]\n",
    "w_init = np.zeros(n_features)\n",
    "b_init = 0.0\n",
    "optimizer = GDOptimizer(learning_rate=0.01)\n",
    "\n",
    "# to save the results from training\n",
    "results = []\n",
    "\n",
    "# metrics\n",
    "metrics_dict = {\n",
    "    \"Accuracy\": lambda y_true, y_pred: accuracy(y_pred, y_true, one_hot_encoded_labels=False),\n",
    "    \"Precision\": precision_score,\n",
    "    \"Recall\": recall_score,\n",
    "}\n",
    "\n",
    "# model configurations\n",
    "model_configs = [\n",
    "    {\n",
    "        'model_class': LogisticRegression,\n",
    "        'model_name': 'Logistic Regression',\n",
    "        'init_params': {\n",
    "            'w': w_init.copy(), \n",
    "            'b': b_init, \n",
    "            'optimizer': GDOptimizer(learning_rate=0.01),\n",
    "            'penalty': 'none', # 'none', 'ridge', 'lasso'\n",
    "            'lam': 0.0\n",
    "        },\n",
    "        'fit_params': {'num_epochs': 20, 'batch_size': 32}\n",
    "    }\n",
    "]\n",
    "\n",
    "# specify the models to train\n",
    "models_to_train = []\n",
    "for config in model_configs:\n",
    "    model_to_train = ModelTraining(\n",
    "        model_class=config['model_class'],\n",
    "        model_name=config['model_name'],\n",
    "        init_params=config['init_params'],\n",
    "        metrics_dict=metrics_dict,\n",
    "        **config['fit_params']\n",
    "    )\n",
    "    models_to_train.append(model_to_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76954a56",
   "metadata": {},
   "source": [
    "### 4.1 Base model with detailed evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f70e91",
   "metadata": {},
   "source": [
    "First train only one model with logistic regression and without cross-validation to analyse that in detail. Then with 5-fold cv for more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b8f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import plot_training_history\n",
    "\n",
    "# training process of one model without cv\n",
    "for model_to_train in models_to_train:\n",
    "    \n",
    "    # train the model using the params defined in the dict\n",
    "    print(100*'=')\n",
    "    print(f\"Model: {model_to_train.model_name}\")\n",
    "    model_to_train.train(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "    \n",
    "    # plot training metrics (accuracy and loss for train/test)\n",
    "    if model_to_train.history:\n",
    "        plot_training_history(model_to_train.history, model_to_train.model_name)\n",
    "        print(100*'=')\n",
    "\n",
    "    # eval on the test set and store results\n",
    "    performance = model_to_train.evaluate(X_test_scaled, y_test)\n",
    "    results.append(performance)\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c737731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results for the simple training run\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d50b506",
   "metadata": {},
   "source": [
    "### 4.2 Compare different regularized versions of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20652731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training import ModelTraining\n",
    "\n",
    "# model configurations\n",
    "logreg_configs = [\n",
    "    {\n",
    "        'model_class': LogisticRegression,\n",
    "        'model_name': 'Logistic Regression (Base)',\n",
    "        'init_params': {\n",
    "            'w': w_init.copy(), \n",
    "            'b': b_init, \n",
    "            'optimizer': GDOptimizer(learning_rate=0.01),\n",
    "            'penalty': 'none',\n",
    "            'lam': 0.0\n",
    "        },\n",
    "        'fit_params': {'num_epochs': 20, 'batch_size': 32}\n",
    "    },\n",
    "    {\n",
    "        'model_class': LogisticRegression,\n",
    "        'model_name': 'Logistic Regression (Weak Ridge)',\n",
    "        'init_params': {\n",
    "            'w': w_init.copy(), \n",
    "            'b': b_init, \n",
    "            'optimizer': GDOptimizer(learning_rate=0.01),\n",
    "            'penalty': 'ridge',\n",
    "            'lam': 0.1\n",
    "        },\n",
    "        'fit_params': {'num_epochs': 20, 'batch_size': 32}\n",
    "    },   \n",
    "    {\n",
    "        'model_class': LogisticRegression,\n",
    "        'model_name': 'Logistic Regression (Strong Ridge)',\n",
    "        'init_params': {\n",
    "            'w': w_init.copy(), \n",
    "            'b': b_init, \n",
    "            'optimizer': GDOptimizer(learning_rate=0.01),\n",
    "            'penalty': 'lasso',\n",
    "            'lam': 1\n",
    "        },\n",
    "        'fit_params': {'num_epochs': 20, 'batch_size': 32}\n",
    "    },\n",
    "    {\n",
    "        'model_class': LogisticRegression,\n",
    "        'model_name': 'Logistic Regression (Lasso)',\n",
    "        'init_params': {\n",
    "            'w': w_init.copy(), \n",
    "            'b': b_init, \n",
    "            'optimizer': GDOptimizer(learning_rate=0.01),\n",
    "            'penalty': 'lasso',\n",
    "            'lam': 0.1\n",
    "        },\n",
    "        'fit_params': {'num_epochs': 20, 'batch_size': 32}\n",
    "    }\n",
    "]\n",
    "\n",
    "# specify the logreg models to train\n",
    "logreg_models_to_train = []\n",
    "for logreg_config in logreg_configs:\n",
    "    logreg_model_to_train = ModelTraining(\n",
    "        model_class=logreg_config['model_class'],\n",
    "        model_name=logreg_config['model_name'],\n",
    "        init_params=logreg_config['init_params'],\n",
    "        metrics_dict=metrics_dict,\n",
    "        plot_cm=False,\n",
    "        **logreg_config['fit_params']\n",
    "    )\n",
    "    logreg_models_to_train.append(logreg_model_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9968980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the models via 5-fold cv now\n",
    "all_cv_results = []\n",
    "for logreg_model_to_train in logreg_models_to_train:\n",
    "    # k-fold cross-validation for current model\n",
    "    cv_result = logreg_model_to_train.cross_validate(X_train_scaled, y_train, k_folds=5)\n",
    "    all_cv_results.append(cv_result)\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b7cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final summary of the cv pf all models tested\n",
    "cv_results_df = pd.DataFrame(all_cv_results)\n",
    "display(cv_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab5e678",
   "metadata": {},
   "source": [
    "#### Why all those metrics\n",
    "- Accuracy can be misleading: a model could achieve ~76% accuracy by simply always predicting the majority class (<=50K), making it useless.\n",
    "- Precision: \"Of all the people we predicted would have high income, how many actually did?\" Important if the cost of a false positive is high.\n",
    "- Recall: \"Of all the people who actually have high income, how many did we find?\" Important if the cost of a false negative is high.\n",
    "- Confusion Matrix: provides the raw numbers (TP, TN, FP, FN) from which all the other metrics are calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36daa766",
   "metadata": {},
   "source": [
    "## 6 Hyperparameter tuning of those models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64637b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch_logistic_regression(X_train, y_train, X_test, y_test, param_grid, cv_folds=None):\n",
    "    \"\"\"\n",
    "    Performs a grid search for Logistic Regression.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # parameter grid\n",
    "    learning_rates = param_grid.get('lr', [0.01])\n",
    "    epochs_list = param_grid.get('num_epochs', [20])\n",
    "    penalties = param_grid.get('penalty', ['none'])\n",
    "    lambdas = param_grid.get('lam', [0.0])\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        for num_epochs in epochs_list:\n",
    "            for penalty in penalties:\n",
    "                for lam in lambdas:\n",
    "                    # Skip non-reg if lam is not 0\n",
    "                    if penalty == 'none' and lam != 0.0:\n",
    "                        continue\n",
    "                    # Skip reg if lam is 0\n",
    "                    if penalty != 'none' and lam == 0.0:\n",
    "                        continue\n",
    "                        \n",
    "                    print(f\"Training with lr={lr}, epochs={num_epochs}, penalty='{penalty}', lambda={lam}\")\n",
    "\n",
    "                    # init model with hyperparameters\n",
    "                    optimizer = GDOptimizer(learning_rate=lr)\n",
    "                    model = LogisticRegression(\n",
    "                        w=np.zeros(X_train.shape[1]),\n",
    "                        b=0.0,\n",
    "                        optimizer=optimizer,\n",
    "                        penalty=penalty,\n",
    "                        lam=lam\n",
    "                    )\n",
    "                    \n",
    "                    # ModelTraining wrapper for training/evaluation\n",
    "                    trainer = ModelTraining(\n",
    "                        model_class=LogisticRegression,\n",
    "                        model_name=\"Tuning\",\n",
    "                        init_params={'w': model.w, 'b': model.b, 'optimizer': optimizer, 'penalty': penalty, 'lam': lam},\n",
    "                        metrics_dict=metrics_dict,\n",
    "                        num_epochs=num_epochs,\n",
    "                        batch_size=32,\n",
    "                        plot_cm=False\n",
    "                    )\n",
    "                    if cv_folds is not None and cv_folds > 1:\n",
    "                        # cross-validation mode\n",
    "                        cv_result = trainer.cross_validate(X_train, y_train, k_folds=cv_folds)\n",
    "                        # add hyperparams to cv_results\n",
    "                        cv_result.update({\n",
    "                            \"Learning Rate\": lr,\n",
    "                            \"Epochs\": num_epochs,\n",
    "                            \"Penalty\": penalty,\n",
    "                            \"Lambda\": lam\n",
    "                            })\n",
    "                        results.append(cv_result)\n",
    "                    else:\n",
    "                        # normal training - no cv\n",
    "                        trainer.train(X_train, y_train)\n",
    "                        performance = trainer.evaluate(X_test, y_test)\n",
    "                        results.append({\n",
    "                            \"Learning Rate\": lr,\n",
    "                            \"Epochs\": num_epochs,\n",
    "                            \"Penalty\": penalty,\n",
    "                            \"Lambda\": lam,\n",
    "                            \"Accuracy\": performance['Accuracy'],\n",
    "                            \"Precision\": performance['Precision'],\n",
    "                            \"Recall\": performance['Recall']\n",
    "                        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# grid of hyperparameters\n",
    "param_grid = {\n",
    "    'lr': [0.01], #, 0.05, 0.1, 0.5],\n",
    "    'num_epochs': [20], #, 30, 50, 100],\n",
    "    'penalty': ['none', 'ridge', 'lasso'],\n",
    "    'lam': [0.0, 0.00001, 0.1], #, 1, 10, 100.0]\n",
    "}\n",
    "\n",
    "\n",
    "logreg_tuning_results = gridsearch_logistic_regression(\n",
    "    X_train_scaled, y_train, X_test_scaled, y_test,\n",
    "    param_grid,\n",
    "    cv_folds=5 # optional param as cv takes a lot of time - can be uncommented for faster performance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f72845",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(logreg_tuning_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36154897",
   "metadata": {},
   "source": [
    "### 6.2 Analyse the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f9f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best model by (mean) accuracy\n",
    "if \"Mean Accuracy\" in logreg_tuning_results.columns:\n",
    "    best_row = logreg_tuning_results.sort_values(\"Mean Accuracy\", ascending=False).iloc[0]\n",
    "    best_acc = best_row[\"Mean Accuracy\"]\n",
    "else:\n",
    "    best_row = logreg_tuning_results.sort_values(\"Accuracy\", ascending=False).iloc[0]\n",
    "    best_acc = best_row[\"Accuracy\"]\n",
    "\n",
    "# get the params of the best model\n",
    "best_penalty = best_row[\"Penalty\"]\n",
    "best_lam = best_row[\"Lambda\"]\n",
    "best_lr = best_row[\"Learning Rate\"]\n",
    "best_epochs = best_row[\"Epochs\"]\n",
    "print(f\"Best model: penalty={best_penalty}, lambda={best_lam}, lr={best_lr}, epochs={best_epochs}\")\n",
    "\n",
    "# retrain on the full training set - no cv here\n",
    "best_optimizer = GDOptimizer(learning_rate=best_lr)\n",
    "best_model = LogisticRegression(\n",
    "    w=np.zeros(X_train_scaled.shape[1]),\n",
    "    b=0.0,\n",
    "    optimizer=best_optimizer,\n",
    "    penalty=best_penalty,\n",
    "    lam=best_lam\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf70eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initial weights:\", best_model.w)\n",
    "print(\"Initial bias:\", best_model.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17abe5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trainer = ModelTraining(\n",
    "    model_class=LogisticRegression,\n",
    "    model_name=\"Best Logistic Regression\",\n",
    "    init_params={'w': best_model.w, 'b': best_model.b, 'optimizer': best_optimizer, 'penalty': best_penalty, 'lam': best_lam},\n",
    "    metrics_dict=metrics_dict,\n",
    "    num_epochs=int(best_epochs),\n",
    "    batch_size=32,\n",
    "    plot_cm=True\n",
    ")\n",
    "best_trainer.train(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "# plot training metrics (accuracy and loss for train/test)\n",
    "if model_to_train.history:\n",
    "    plot_training_history(best_trainer.history, best_trainer.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a6f12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Weights after training:\", best_trainer.model.w)\n",
    "print(\"Bias after training:\", best_trainer.model.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb7bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze coefficients\n",
    "coefs = best_trainer.model.w\n",
    "feature_names = [col for col in train_df_encoded.columns if col != 'income']\n",
    "coef_df = pd.DataFrame({'feature': feature_names, 'coefficient': coefs})\n",
    "# 10 most important features\n",
    "top_pos = coef_df.sort_values(by='coefficient', ascending=False).head(5)\n",
    "top_neg = coef_df.sort_values(by='coefficient').head(5)\n",
    "print(\"Top 5 Positive Coefficients (features that increase prob of >50K):\")\n",
    "print(top_pos)\n",
    "print(\"\\nTop 5 Negative Coefficients (features that decrease prob of >50K):\")\n",
    "print(top_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7a6b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the importance of all features for our best model, sorted by absolute value\n",
    "coef_df_sorted = coef_df.reindex(coef_df['coefficient'].sort_values(ascending=False).index)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(coef_df_sorted['feature'], coef_df_sorted['coefficient'], color=['#00246B' if c > 0 else '#CADCFC' for c in coef_df_sorted['coefficient']])\n",
    "plt.xlabel('Coefficient Value in Logistic Regression')\n",
    "plt.title('Importance of the Features on the Probability of >$50K Income')\n",
    "plt.axvline(0, color='black', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c1ccd4",
   "metadata": {},
   "source": [
    "Intrepreting the negative coefficient on is_from_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d4220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.groupby('is_from_us')['income'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0640e3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# does this stem from multicollinearity?\n",
    "corr = train_df_encoded.corr()\n",
    "print(corr['is_from_us'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6df2e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency of non-US and income\n",
    "print(train_df_encoded.groupby('is_from_us')['income'].mean())\n",
    "print(train_df_encoded['income'].value_counts())\n",
    "print(train_df_encoded['is_from_us'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aead5d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.glm import LogisticRegression\n",
    "from optimizers import GDOptimizer\n",
    "\n",
    "# small model only on people from the US \n",
    "X_us = train_df_encoded[['is_from_us']].values\n",
    "y = train_df_encoded['income'].values\n",
    "model = LogisticRegression(w=np.zeros(1), b=0.0, optimizer=GDOptimizer(learning_rate=0.01))\n",
    "for _ in range(100):\n",
    "    grads = model.loss_grad(X_us, y)\n",
    "    model.w -= 0.01 * grads['w']\n",
    "    model.b -= 0.01 * grads['b']\n",
    "print(\"Coefficient for is_from_us (only feature in model):\", model.w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53615290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(fit_intercept=True)\n",
    "model.fit(train_df[['is_from_us']], train_df['income'])\n",
    "print(model.coef_, model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b611dd4",
   "metadata": {},
   "source": [
    "### Interpreting Odds Ratios for Logistic Regression Coefficients\n",
    "\n",
    "This shows the **odds ratios** for each feature in the logistic regression model. The odds ratio is calculated as `exp(coefficient)` and represents the multiplicative change in the odds of earning >$50K for a one-unit increase in the feature, holding all other features constant:\n",
    "\n",
    "- **Odds Ratio > 1:** The feature increases the odds of high income. For example, an odds ratio of 2 means the odds double for each unit increase.\n",
    "- **Odds Ratio < 1:** The feature decreases the odds of high income. For example, an odds ratio of 0.5 means the odds are halved for each unit increase.\n",
    "- **Odds Ratio â‰ˆ 1:** The feature has little or no effect on the odds.\n",
    "\n",
    "#### What is a \"unit increase\" for each feature?\n",
    "\n",
    "- **Standardized numerical features**: unit increase means **one standard deviation above the mean** of that feature .\n",
    "  - For `capital-gain` and `capital-loss`, this is after a `log1p` transformation, so it reflects a standard deviation in the log-transformed value.\n",
    "\n",
    "- **Binary features**: changing from 0 to 1 (e.g., from not married to married).\n",
    "\n",
    "- **One-hot encoded features**: unit increase means belonging to that category versus the baseline (reference) category.\n",
    "\n",
    "- **Ordinally encoded features**: unit increase means moving up by one level in the encoded order, but after standardization, it is also in units of standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8858d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odds ratios for the features\n",
    "coef_df[\"odds_ratio\"] = np.exp(coef_df[\"coefficient\"])\n",
    "print(\"Top features with odds ratios:\")\n",
    "display(coef_df.loc[coef_df.index][[\"feature\", \"coefficient\", \"odds_ratio\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applied_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
